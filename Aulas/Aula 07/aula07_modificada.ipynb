{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNUZqT8Se+wlV8/ceAV5m+3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6fd3631b0dd842e6b2e38e46bcc9a98b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d215c4a6bc8c450ea27372b2f965c49d","IPY_MODEL_3fb1f0cea01249f6b26472ce09e897a1","IPY_MODEL_a3c40744288c454fa40960bc8074b36e"],"layout":"IPY_MODEL_b1afa80b5eb1473192976f4a2cc778df"}},"d215c4a6bc8c450ea27372b2f965c49d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9493285b49354e2c8255329d293a65f1","placeholder":"​","style":"IPY_MODEL_2f3fa9d2517c436db018f210bf2bd985","value":"Best trial: 0. Best value: 0.672253:   3%"}},"3fb1f0cea01249f6b26472ce09e897a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f7b9b5a9d134390958c4b1b187b8fee","max":30,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b969e081bf9943339f58a9872b737678","value":1}},"a3c40744288c454fa40960bc8074b36e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb254f489758497ba9efdc0b379cd0b1","placeholder":"​","style":"IPY_MODEL_f2eb9930c22740b19f1e6a694861d508","value":" 1/30 [01:45&lt;50:53, 105.29s/it]"}},"b1afa80b5eb1473192976f4a2cc778df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9493285b49354e2c8255329d293a65f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f3fa9d2517c436db018f210bf2bd985":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f7b9b5a9d134390958c4b1b187b8fee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b969e081bf9943339f58a9872b737678":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb254f489758497ba9efdc0b379cd0b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2eb9930c22740b19f1e6a694861d508":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0vCENSvHhiI","executionInfo":{"status":"ok","timestamp":1749765151614,"user_tz":180,"elapsed":46199,"user":{"displayName":"Matheus Pavani","userId":"01895380900599612817"}},"outputId":"80c83ec9-9ca0-495c-a676-95177e569131"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset gerado com sucesso!\n","Shape: (1000000, 35)\n","\n","Primeiras 5 linhas:\n","   Age Attrition     BusinessTravel  DailyRate              Department  \\\n","0   42        No      Travel_Rarely        575  Research & Development   \n","1   36        No      Travel_Rarely        535         Human Resources   \n","2   44        No      Travel_Rarely        286  Research & Development   \n","3   53       Yes  Travel_Frequently        746                   Sales   \n","4   35        No      Travel_Rarely        408  Research & Development   \n","\n","   DistanceFromHome  Education    EducationField  EmployeeCount  \\\n","0                 1          3         Marketing              1   \n","1                 2          4   Human Resources              1   \n","2                 2          4           Medical              1   \n","3                11          2           Medical              1   \n","4                 7          5  Technical Degree              1   \n","\n","   EmployeeNumber  ...  RelationshipSatisfaction StandardHours  \\\n","0               1  ...                         2            80   \n","1               2  ...                         4            80   \n","2               3  ...                         1            80   \n","3               4  ...                         4            80   \n","4               5  ...                         2            80   \n","\n","   StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  \\\n","0                 1                 24                      3               4   \n","1                 1                 18                      5               2   \n","2                 2                 22                      3               3   \n","3                 0                 31                      1               3   \n","4                 1                 17                      2               1   \n","\n","   YearsAtCompany YearsInCurrentRole  YearsSinceLastPromotion  \\\n","0               5                  0                        3   \n","1              17                  8                        7   \n","2              20                  0                        1   \n","3               3                  3                        0   \n","4              14                  5                        3   \n","\n","   YearsWithCurrManager  \n","0                     0  \n","1                     3  \n","2                     0  \n","3                     3  \n","4                     5  \n","\n","[5 rows x 35 columns]\n","\n","Estatísticas de Attrition:\n","Attrition\n","No     0.821159\n","Yes    0.178841\n","Name: proportion, dtype: float64\n","\n","Distribuição de JobLevel:\n","JobLevel\n","1     10453\n","2    228565\n","3    556401\n","4    199027\n","5      5554\n","Name: count, dtype: int64\n","\n","Média de salário por JobLevel:\n","JobLevel\n","1     4978.61\n","2     8156.51\n","3    11428.82\n","4    14746.94\n","5    18242.11\n","Name: MonthlyIncome, dtype: float64\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","def generate_ibm_hr_dataset(n_samples=1_000_000, seed=42):\n","    \"\"\"\n","    Gera um dataset sintético inspirado no IBM HR Analytics com relações mais realistas\n","\n","    Parameters:\n","    n_samples (int): Número de amostras a gerar\n","    seed (int): Seed para reprodutibilidade\n","\n","    Returns:\n","    pd.DataFrame: Dataset gerado\n","    \"\"\"\n","    np.random.seed(seed)\n","\n","    # Gerando dados base\n","    data = {}\n","\n","    # Idade com distribuição mais realista (concentrada entre 25-50 anos)\n","    data['Age'] = np.random.normal(38, 10, n_samples).astype(int)\n","    data['Age'] = np.clip(data['Age'], 18, 65)\n","\n","    # Gênero\n","    data['Gender'] = np.random.choice(['Female', 'Male'], n_samples, p=[0.4, 0.6])\n","\n","    # Educação (1-5: Below College, College, Bachelor, Master, Doctor)\n","    education_probs = [0.05, 0.15, 0.40, 0.30, 0.10]\n","    data['Education'] = np.random.choice([1, 2, 3, 4, 5], n_samples, p=education_probs)\n","\n","    # Campo educacional baseado no nível de educação\n","    education_fields = ['Life Sciences', 'Other', 'Medical', 'Marketing', 'Technical Degree', 'Human Resources']\n","    data['EducationField'] = np.random.choice(education_fields, n_samples)\n","\n","    # Departamento\n","    dept_probs = [0.45, 0.40, 0.15]  # Sales mais comum, HR menos comum\n","    data['Department'] = np.random.choice(['Sales', 'Research & Development', 'Human Resources'],\n","                                        n_samples, p=dept_probs)\n","\n","    # Job Role baseado no departamento\n","    job_roles = {\n","        'Sales': ['Sales Executive', 'Sales Representative', 'Manager'],\n","        'Research & Development': ['Research Scientist', 'Laboratory Technician', 'Research Director', 'Manager'],\n","        'Human Resources': ['Human Resources', 'Manager']\n","    }\n","\n","    data['JobRole'] = np.empty(n_samples, dtype=object)\n","    for dept in job_roles:\n","        mask = np.array(data['Department']) == dept\n","        n_dept = mask.sum()\n","        if n_dept > 0:\n","            data['JobRole'][mask] = np.random.choice(job_roles[dept], n_dept)\n","\n","    # Job Level correlacionado com idade e educação\n","    base_level = np.ones(n_samples)\n","    age_bonus = (np.array(data['Age']) - 18) / 47 * 2  # 0-2 pontos baseado na idade\n","    edu_bonus = (np.array(data['Education']) - 1) / 4 * 2  # 0-2 pontos baseado na educação\n","\n","    data['JobLevel'] = np.round(base_level + age_bonus + edu_bonus).astype(int)\n","    data['JobLevel'] = np.clip(data['JobLevel'], 1, 5)\n","\n","    # Total Working Years correlacionado com idade\n","    data['TotalWorkingYears'] = np.maximum(0, data['Age'] - 18 - np.random.randint(0, 5, n_samples))\n","\n","    # Years at Company (não pode ser maior que TotalWorkingYears)\n","    data['YearsAtCompany'] = np.random.randint(0, 21, n_samples)\n","    data['YearsAtCompany'] = np.minimum(data['YearsAtCompany'], data['TotalWorkingYears'])\n","\n","    # Years in Current Role (não pode ser maior que YearsAtCompany)\n","    data['YearsInCurrentRole'] = np.random.randint(0, 11, n_samples)\n","    data['YearsInCurrentRole'] = np.minimum(data['YearsInCurrentRole'], data['YearsAtCompany'])\n","\n","    # Years Since Last Promotion\n","    data['YearsSinceLastPromotion'] = np.random.randint(0, 8, n_samples)\n","    data['YearsSinceLastPromotion'] = np.minimum(data['YearsSinceLastPromotion'], data['YearsAtCompany'])\n","\n","    # Years With Current Manager\n","    data['YearsWithCurrManager'] = np.random.randint(0, 8, n_samples)\n","    data['YearsWithCurrManager'] = np.minimum(data['YearsWithCurrManager'], data['YearsInCurrentRole'])\n","\n","    # Número de empresas trabalhadas (correlacionado com anos totais de trabalho)\n","    max_companies = np.minimum(data['TotalWorkingYears'] // 2, 9)\n","    data['NumCompaniesWorked'] = np.array([np.random.randint(0, max(1, mc) + 1) for mc in max_companies])\n","\n","    # Monthly Income correlacionado com JobLevel, Education e TotalWorkingYears\n","    base_income = 2000\n","    level_factor = data['JobLevel'] * 2000\n","    education_factor = data['Education'] * 500\n","    experience_factor = data['TotalWorkingYears'] * 100\n","    noise = np.random.normal(0, 1000, n_samples)\n","\n","    data['MonthlyIncome'] = base_income + level_factor + education_factor + experience_factor + noise\n","    data['MonthlyIncome'] = np.clip(data['MonthlyIncome'].astype(int), 1000, 20000)\n","\n","    # Rates\n","    data['DailyRate'] = np.random.randint(100, 1500, n_samples)\n","    data['HourlyRate'] = np.random.randint(30, 100, n_samples)\n","    data['MonthlyRate'] = np.random.randint(2000, 27000, n_samples)\n","\n","    # Distance from home (distribuição exponencial - mais pessoas moram perto)\n","    data['DistanceFromHome'] = np.random.exponential(7, n_samples).astype(int) + 1\n","    data['DistanceFromHome'] = np.clip(data['DistanceFromHome'], 1, 29)\n","\n","    # Business Travel\n","    travel_probs = [0.70, 0.20, 0.10]  # Maioria viaja raramente\n","    data['BusinessTravel'] = np.random.choice(['Travel_Rarely', 'Travel_Frequently', 'Non-Travel'],\n","                                            n_samples, p=travel_probs)\n","\n","    # Satisfação e envolvimento\n","    data['EnvironmentSatisfaction'] = np.random.choice([1, 2, 3, 4], n_samples, p=[0.10, 0.20, 0.40, 0.30])\n","    data['JobSatisfaction'] = np.random.choice([1, 2, 3, 4], n_samples, p=[0.10, 0.20, 0.40, 0.30])\n","    data['RelationshipSatisfaction'] = np.random.choice([1, 2, 3, 4], n_samples, p=[0.10, 0.20, 0.40, 0.30])\n","    data['JobInvolvement'] = np.random.choice([1, 2, 3, 4], n_samples, p=[0.05, 0.15, 0.50, 0.30])\n","    data['WorkLifeBalance'] = np.random.choice([1, 2, 3, 4], n_samples, p=[0.10, 0.25, 0.45, 0.20])\n","\n","    # Performance Rating (maioria boa performance)\n","    data['PerformanceRating'] = np.random.choice([3, 4], n_samples, p=[0.84, 0.16])\n","\n","    # Percent Salary Hike correlacionado com Performance Rating\n","    data['PercentSalaryHike'] = np.where(\n","        data['PerformanceRating'] == 4,\n","        np.random.randint(15, 26, n_samples),\n","        np.random.randint(11, 18, n_samples)\n","    )\n","\n","    # Stock Option Level correlacionado com JobLevel\n","    data['StockOptionLevel'] = np.random.choice([0, 1, 2, 3], n_samples,\n","                                               p=[0.40, 0.35, 0.20, 0.05])\n","    high_level_mask = data['JobLevel'] >= 4\n","    data['StockOptionLevel'][high_level_mask] = np.random.choice([1, 2, 3],\n","                                                                 high_level_mask.sum(),\n","                                                                 p=[0.30, 0.50, 0.20])\n","\n","    # Training Times Last Year\n","    data['TrainingTimesLastYear'] = np.random.choice([0, 1, 2, 3, 4, 5, 6], n_samples,\n","                                                    p=[0.05, 0.10, 0.25, 0.30, 0.20, 0.08, 0.02])\n","\n","    # Marital Status\n","    data['MaritalStatus'] = np.random.choice(['Single', 'Married', 'Divorced'], n_samples,\n","                                           p=[0.32, 0.55, 0.13])\n","\n","    # OverTime - maior probabilidade para níveis menores e pessoas mais jovens\n","    overtime_base_prob = 0.28\n","    age_factor = (65 - data['Age']) / 47 * 0.1  # Jovens trabalham mais overtime\n","    level_factor = (5 - data['JobLevel']) / 4 * 0.1  # Níveis menores trabalham mais overtime\n","\n","    overtime_prob = np.clip(overtime_base_prob + age_factor + level_factor, 0.1, 0.5)\n","    data['OverTime'] = [np.random.choice(['Yes', 'No'], p=[p, 1-p]) for p in overtime_prob]\n","\n","    # Attrition - baseado em múltiplos fatores\n","    attrition_score = np.zeros(n_samples)\n","\n","    # Fatores que aumentam attrition\n","    attrition_score += (data['JobSatisfaction'] == 1) * 0.15\n","    attrition_score += (data['EnvironmentSatisfaction'] == 1) * 0.10\n","    attrition_score += (data['WorkLifeBalance'] == 1) * 0.10\n","    attrition_score += (np.array(data['OverTime']) == 'Yes') * 0.08\n","    attrition_score += (data['YearsSinceLastPromotion'] > 5) * 0.05\n","    attrition_score += (data['DistanceFromHome'] > 20) * 0.05\n","    attrition_score += (np.array(data['MaritalStatus']) == 'Single') * 0.03\n","    attrition_score += (data['NumCompaniesWorked'] > 5) * 0.04\n","\n","    # Fatores que diminuem attrition\n","    attrition_score -= (data['JobLevel'] >= 4) * 0.10\n","    attrition_score -= (data['YearsAtCompany'] > 10) * 0.08\n","    attrition_score -= (data['StockOptionLevel'] > 0) * 0.05\n","\n","    # Probabilidade base de 16%\n","    attrition_prob = np.clip(0.16 + attrition_score, 0.05, 0.50)\n","    data['Attrition'] = [np.random.choice(['Yes', 'No'], p=[p, 1-p]) for p in attrition_prob]\n","\n","    # Campos fixos\n","    data['EmployeeCount'] = np.ones(n_samples, dtype=int)\n","    data['EmployeeNumber'] = np.arange(1, n_samples + 1)\n","    data['Over18'] = ['Y'] * n_samples\n","    data['StandardHours'] = [80] * n_samples\n","\n","    # Criar DataFrame\n","    df = pd.DataFrame(data)\n","\n","    # Reordenar colunas para match com o dataset original\n","    column_order = [\n","        'Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n","        'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount',\n","        'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate',\n","        'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n","        'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n","        'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n","        'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel',\n","        'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n","        'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n","        'YearsWithCurrManager'\n","    ]\n","\n","    return df[column_order]\n","\n","# Exemplo de uso\n","if __name__ == \"__main__\":\n","    # Gerar dataset\n","    df = generate_ibm_hr_dataset(n_samples=1_000_000, seed=42)\n","\n","    print(\"Dataset gerado com sucesso!\")\n","    print(f\"Shape: {df.shape}\")\n","    print(\"\\nPrimeiras 5 linhas:\")\n","    print(df.head())\n","\n","    print(\"\\nEstatísticas de Attrition:\")\n","    print(df['Attrition'].value_counts(normalize=True))\n","\n","    print(\"\\nDistribuição de JobLevel:\")\n","    print(df['JobLevel'].value_counts().sort_index())\n","\n","    print(\"\\nMédia de salário por JobLevel:\")\n","    print(df.groupby('JobLevel')['MonthlyIncome'].mean().round(2))"]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYNlciuzKxeA","executionInfo":{"status":"ok","timestamp":1749765802296,"user_tz":180,"elapsed":6836,"user":{"displayName":"Matheus Pavani","userId":"01895380900599612817"}},"outputId":"8a5aaabd-6678-4478-b87a-3bed40e72df2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n","Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (classification_report, confusion_matrix,\n","                           roc_auc_score, roc_curve, precision_recall_curve,\n","                           f1_score, accuracy_score, precision_score, recall_score)\n","import lightgbm as lgb\n","import optuna\n","from optuna.samplers import TPESampler\n","import warnings\n","import time\n","from datetime import datetime\n","import joblib\n","import os\n","\n","warnings.filterwarnings('ignore')\n","\n","# Configurar matplotlib para melhor visualização\n","plt.style.use('seaborn-v0_8-darkgrid')\n","plt.rcParams['figure.figsize'] = (12, 8)\n","plt.rcParams['font.size'] = 12"],"metadata":{"id":"UGiLQ_q7IKTb","executionInfo":{"status":"ok","timestamp":1749765805557,"user_tz":180,"elapsed":1,"user":{"displayName":"Matheus Pavani","userId":"01895380900599612817"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class HRAttritionPipeline:\n","    \"\"\"Pipeline completo para previsão de attrition em RH\"\"\"\n","\n","    def __init__(self, use_gpu=True):\n","        self.use_gpu = use_gpu\n","        self.models = {}\n","        self.results = {}\n","        self.preprocessor = None\n","        self.feature_names = None\n","\n","    def load_and_prepare_data(self, df):\n","        \"\"\"Carrega e prepara os dados para modelagem\"\"\"\n","        print(\"=== Preparação dos Dados ===\")\n","        print(f\"Shape inicial: {df.shape}\")\n","\n","        # Remover colunas com valores únicos\n","        single_value_cols = ['EmployeeCount', 'Over18', 'StandardHours']\n","        df = df.drop(columns=single_value_cols)\n","        print(f\"Colunas removidas (valor único): {single_value_cols}\")\n","\n","        # Remover EmployeeNumber (ID único)\n","        df = df.drop(columns=['EmployeeNumber'])\n","\n","        # Separar features e target\n","        X = df.drop('Attrition', axis=1)\n","        y = df['Attrition'].map({'Yes': 1, 'No': 0})\n","\n","        print(f\"\\nDistribuição do target:\")\n","        print(y.value_counts(normalize=True))\n","\n","        return X, y\n","\n","    def create_features(self, X):\n","        \"\"\"Cria features adicionais através de feature engineering\"\"\"\n","        print(\"\\n=== Feature Engineering ===\")\n","        X_eng = X.copy()\n","\n","        # Ratios e proporções\n","        X_eng['IncomeToAge'] = X_eng['MonthlyIncome'] / X_eng['Age']\n","        X_eng['YearsPerCompany'] = X_eng['TotalWorkingYears'] / (X_eng['NumCompaniesWorked'] + 1)\n","        X_eng['YearsWithoutPromotion'] = X_eng['YearsSinceLastPromotion'] / (X_eng['YearsAtCompany'] + 1)\n","        X_eng['IncomeToEducation'] = X_eng['MonthlyIncome'] / X_eng['Education']\n","        X_eng['DistanceSatisfactionRatio'] = X_eng['DistanceFromHome'] / (X_eng['EnvironmentSatisfaction'] + 1)\n","\n","        # Indicadores binários\n","        X_eng['HighDistance'] = (X_eng['DistanceFromHome'] > 20).astype(int)\n","        X_eng['LowSatisfaction'] = ((X_eng['JobSatisfaction'] == 1) |\n","                                    (X_eng['EnvironmentSatisfaction'] == 1) |\n","                                    (X_eng['RelationshipSatisfaction'] == 1)).astype(int)\n","        X_eng['NoPromotion'] = (X_eng['YearsSinceLastPromotion'] > 5).astype(int)\n","        X_eng['HighPerformer'] = (X_eng['PerformanceRating'] == 4).astype(int)\n","        X_eng['YoungHighLevel'] = ((X_eng['Age'] < 30) & (X_eng['JobLevel'] >= 3)).astype(int)\n","\n","        # Scores compostos\n","        X_eng['SatisfactionScore'] = (X_eng['JobSatisfaction'] +\n","                                      X_eng['EnvironmentSatisfaction'] +\n","                                      X_eng['RelationshipSatisfaction']) / 3\n","\n","        X_eng['WorkIntensity'] = (X_eng['OverTime'].map({'Yes': 2, 'No': 1}) *\n","                                  X_eng['BusinessTravel'].map({'Non-Travel': 1,\n","                                                              'Travel_Rarely': 2,\n","                                                              'Travel_Frequently': 3}))\n","\n","        # Categorias de idade\n","        X_eng['AgeGroup'] = pd.cut(X_eng['Age'], bins=[0, 25, 35, 45, 55, 100],\n","                                  labels=['<25', '25-35', '35-45', '45-55', '55+'])\n","\n","        # Categorias de income\n","        X_eng['IncomeGroup'] = pd.qcut(X_eng['MonthlyIncome'], q=5,\n","                                      labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'])\n","\n","        print(f\"Features criadas: {X_eng.shape[1] - X.shape[1]}\")\n","        print(f\"Total de features: {X_eng.shape[1]}\")\n","\n","        return X_eng\n","\n","    def create_preprocessor(self, X):\n","        \"\"\"Cria o preprocessador para as features\"\"\"\n","        # Identificar colunas por tipo\n","        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n","        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n","\n","        print(f\"\\nFeatures numéricas: {len(numeric_features)}\")\n","        print(f\"Features categóricas: {len(categorical_features)}\")\n","\n","        # Criar transformadores\n","        numeric_transformer = StandardScaler()\n","        categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n","\n","        # Combinar transformadores\n","        preprocessor = ColumnTransformer(\n","            transformers=[\n","                ('num', numeric_transformer, numeric_features),\n","                ('cat', categorical_transformer, categorical_features)\n","            ])\n","\n","        return preprocessor, numeric_features, categorical_features\n","\n","    def optimize_hyperparameters(self, X_train, y_train, model_name, n_trials=50):\n","        \"\"\"Otimiza hiperparâmetros usando Optuna\"\"\"\n","        print(f\"\\n=== Otimizando {model_name} ===\")\n","\n","        # Converter para numpy array se necessário\n","        if not isinstance(X_train, np.ndarray):\n","            X_train = np.array(X_train)\n","        if not isinstance(y_train, np.ndarray):\n","            y_train = np.array(y_train)\n","\n","        # Preparar dados para validação cruzada\n","        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","        def objective(trial):\n","            if model_name == 'RandomForest':\n","                params = {\n","                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n","                    'max_depth': trial.suggest_int('max_depth', 3, 20),\n","                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n","                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n","                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n","                    'random_state': 42,\n","                    'n_jobs': -1\n","                }\n","                model = RandomForestClassifier(**params)\n","\n","            elif model_name == 'LogisticRegression':\n","                params = {\n","                    'C': trial.suggest_float('C', 0.001, 10.0, log=True),\n","                    'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n","                    'solver': 'liblinear' if trial.params['penalty'] == 'l1' else 'lbfgs',\n","                    'max_iter': 1000,\n","                    'random_state': 42,\n","                    'n_jobs': -1\n","                }\n","                model = LogisticRegression(**params)\n","\n","            elif model_name == 'LightGBM':\n","                params = {\n","                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n","                    'max_depth': trial.suggest_int('max_depth', 3, 20),\n","                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n","                    'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n","                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n","                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n","                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n","                    'random_state': 42,\n","                    'n_jobs': -1,\n","                    'verbose': -1\n","                }\n","\n","                if self.use_gpu:\n","                    params['device'] = 'gpu'\n","                    params['gpu_use_dp'] = False\n","\n","                model = lgb.LGBMClassifier(**params)\n","\n","            # Validação cruzada\n","            scores = []\n","            for train_idx, val_idx in cv.split(X_train, y_train):\n","                # Usar indexação numpy correta\n","                X_fold_train = X_train[train_idx, :]\n","                y_fold_train = y_train[train_idx]\n","                X_fold_val = X_train[val_idx, :]\n","                y_fold_val = y_train[val_idx]\n","\n","                model.fit(X_fold_train, y_fold_train)\n","                y_pred = model.predict_proba(X_fold_val)[:, 1]\n","                score = roc_auc_score(y_fold_val, y_pred)\n","                scores.append(score)\n","\n","            return np.mean(scores)\n","\n","        # Otimizar\n","        study = optuna.create_study(\n","            direction='maximize',\n","            sampler=TPESampler(seed=42)\n","        )\n","\n","        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n","\n","        print(f\"Melhor AUC-ROC: {study.best_value:.4f}\")\n","        print(f\"Melhores parâmetros: {study.best_params}\")\n","\n","        return study.best_params\n","\n","    def train_models(self, X_train, y_train, X_test, y_test):\n","        \"\"\"Treina todos os modelos com hiperparâmetros otimizados\"\"\"\n","        print(\"\\n=== Treinamento dos Modelos ===\")\n","\n","        # Preprocessar dados\n","        self.preprocessor, self.numeric_features, self.categorical_features = self.create_preprocessor(X_train)\n","\n","        X_train_processed = self.preprocessor.fit_transform(X_train)\n","        X_test_processed = self.preprocessor.transform(X_test)\n","\n","        # Converter para numpy arrays se necessário\n","        if hasattr(X_train_processed, 'toarray'):\n","            X_train_processed = X_train_processed.toarray()\n","        if hasattr(X_test_processed, 'toarray'):\n","            X_test_processed = X_test_processed.toarray()\n","\n","        # Armazenar nomes das features após one-hot encoding\n","        cat_encoder = self.preprocessor.named_transformers_['cat']\n","        cat_feature_names = []\n","        if hasattr(cat_encoder, 'get_feature_names_out'):\n","            cat_feature_names = cat_encoder.get_feature_names_out(self.categorical_features).tolist()\n","\n","        self.feature_names = self.numeric_features + cat_feature_names\n","\n","        # Modelos a treinar\n","        models_config = {\n","            'RandomForest': RandomForestClassifier,\n","            'LogisticRegression': LogisticRegression,\n","            'LightGBM': lgb.LGBMClassifier\n","        }\n","\n","        for model_name, ModelClass in models_config.items():\n","            print(f\"\\n--- {model_name} ---\")\n","            start_time = time.time()\n","\n","            # Otimizar hiperparâmetros\n","            best_params = self.optimize_hyperparameters(\n","                X_train_processed, y_train, model_name, n_trials=30\n","            )\n","\n","            # Treinar modelo final\n","            if model_name == 'LightGBM' and self.use_gpu:\n","                best_params['device'] = 'gpu'\n","                best_params['gpu_use_dp'] = False\n","\n","            model = ModelClass(**best_params)\n","            model.fit(X_train_processed, y_train)\n","\n","            # Fazer predições\n","            y_pred = model.predict(X_test_processed)\n","            y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n","\n","            # Calcular métricas\n","            metrics = {\n","                'accuracy': accuracy_score(y_test, y_pred),\n","                'precision': precision_score(y_test, y_pred),\n","                'recall': recall_score(y_test, y_pred),\n","                'f1': f1_score(y_test, y_pred),\n","                'auc_roc': roc_auc_score(y_test, y_pred_proba),\n","                'training_time': time.time() - start_time\n","            }\n","\n","            # Armazenar resultados\n","            self.models[model_name] = model\n","            self.results[model_name] = {\n","                'metrics': metrics,\n","                'predictions': y_pred,\n","                'probabilities': y_pred_proba,\n","                'best_params': best_params\n","            }\n","\n","            print(f\"AUC-ROC: {metrics['auc_roc']:.4f}\")\n","            print(f\"Tempo de treinamento: {metrics['training_time']:.2f}s\")\n","\n","    def plot_results(self, X_test, y_test):\n","        \"\"\"Gera visualizações comparativas dos modelos\"\"\"\n","        print(\"\\n=== Gerando Visualizações ===\")\n","\n","        fig = plt.figure(figsize=(20, 15))\n","\n","        # 1. Comparação de métricas\n","        ax1 = plt.subplot(3, 3, 1)\n","        metrics_df = pd.DataFrame({\n","            model: results['metrics']\n","            for model, results in self.results.items()\n","        }).T\n","        metrics_df[['accuracy', 'precision', 'recall', 'f1', 'auc_roc']].plot(\n","            kind='bar', ax=ax1, colormap='viridis'\n","        )\n","        ax1.set_title('Comparação de Métricas por Modelo', fontsize=14, fontweight='bold')\n","        ax1.set_xlabel('Modelo')\n","        ax1.set_ylabel('Score')\n","        ax1.legend(loc='lower right')\n","        ax1.grid(True, alpha=0.3)\n","\n","        # 2. Curvas ROC\n","        ax2 = plt.subplot(3, 3, 2)\n","        for model_name, results in self.results.items():\n","            fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n","            auc = results['metrics']['auc_roc']\n","            ax2.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)\n","\n","        ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n","        ax2.set_xlabel('Taxa de Falsos Positivos')\n","        ax2.set_ylabel('Taxa de Verdadeiros Positivos')\n","        ax2.set_title('Curvas ROC', fontsize=14, fontweight='bold')\n","        ax2.legend()\n","        ax2.grid(True, alpha=0.3)\n","\n","        # 3. Curvas Precision-Recall\n","        ax3 = plt.subplot(3, 3, 3)\n","        for model_name, results in self.results.items():\n","            precision, recall, _ = precision_recall_curve(y_test, results['probabilities'])\n","            ax3.plot(recall, precision, label=model_name, linewidth=2)\n","\n","        ax3.set_xlabel('Recall')\n","        ax3.set_ylabel('Precision')\n","        ax3.set_title('Curvas Precision-Recall', fontsize=14, fontweight='bold')\n","        ax3.legend()\n","        ax3.grid(True, alpha=0.3)\n","\n","        # 4-6. Matrizes de Confusão\n","        for idx, (model_name, results) in enumerate(self.results.items()):\n","            ax = plt.subplot(3, 3, 4 + idx)\n","            cm = confusion_matrix(y_test, results['predictions'])\n","            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n","                       xticklabels=['Não', 'Sim'], yticklabels=['Não', 'Sim'])\n","            ax.set_title(f'Matriz de Confusão - {model_name}', fontsize=12)\n","            ax.set_xlabel('Predito')\n","            ax.set_ylabel('Real')\n","\n","        # 7. Feature Importance (Random Forest)\n","        ax7 = plt.subplot(3, 3, 7)\n","        if 'RandomForest' in self.models:\n","            rf_model = self.models['RandomForest']\n","            importances = rf_model.feature_importances_\n","            indices = np.argsort(importances)[::-1][:15]\n","\n","            ax7.barh(range(15), importances[indices], color='skyblue', edgecolor='navy')\n","            ax7.set_yticks(range(15))\n","            ax7.set_yticklabels([self.feature_names[i] for i in indices])\n","            ax7.set_xlabel('Importância')\n","            ax7.set_title('Top 15 Features - Random Forest', fontsize=12)\n","            ax7.grid(True, alpha=0.3, axis='x')\n","\n","        # 8. Feature Importance (LightGBM)\n","        ax8 = plt.subplot(3, 3, 8)\n","        if 'LightGBM' in self.models:\n","            lgb_model = self.models['LightGBM']\n","            importances = lgb_model.feature_importances_\n","            indices = np.argsort(importances)[::-1][:15]\n","\n","            ax8.barh(range(15), importances[indices], color='lightcoral', edgecolor='darkred')\n","            ax8.set_yticks(range(15))\n","            ax8.set_yticklabels([self.feature_names[i] for i in indices])\n","            ax8.set_xlabel('Importância')\n","            ax8.set_title('Top 15 Features - LightGBM', fontsize=12)\n","            ax8.grid(True, alpha=0.3, axis='x')\n","\n","        # 9. Tempo de treinamento\n","        ax9 = plt.subplot(3, 3, 9)\n","        models = list(self.results.keys())\n","        times = [self.results[m]['metrics']['training_time'] for m in models]\n","        bars = ax9.bar(models, times, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n","        ax9.set_xlabel('Modelo')\n","        ax9.set_ylabel('Tempo (segundos)')\n","        ax9.set_title('Tempo de Treinamento', fontsize=12)\n","        ax9.grid(True, alpha=0.3, axis='y')\n","\n","        # Adicionar valores nas barras\n","        for bar, time in zip(bars, times):\n","            height = bar.get_height()\n","            ax9.text(bar.get_x() + bar.get_width()/2., height,\n","                    f'{time:.1f}s', ha='center', va='bottom')\n","\n","        plt.tight_layout()\n","        plt.savefig('model_comparison_results.png', dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def generate_report(self):\n","        \"\"\"Gera relatório detalhado dos resultados\"\"\"\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"RELATÓRIO FINAL - COMPARAÇÃO DE MODELOS\")\n","        print(\"=\"*80)\n","        print(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","        print(\"\\n\")\n","\n","        # Tabela de métricas\n","        metrics_data = []\n","        for model_name, results in self.results.items():\n","            metrics = results['metrics']\n","            metrics_data.append({\n","                'Modelo': model_name,\n","                'Acurácia': f\"{metrics['accuracy']:.4f}\",\n","                'Precisão': f\"{metrics['precision']:.4f}\",\n","                'Recall': f\"{metrics['recall']:.4f}\",\n","                'F1-Score': f\"{metrics['f1']:.4f}\",\n","                'AUC-ROC': f\"{metrics['auc_roc']:.4f}\",\n","                'Tempo (s)': f\"{metrics['training_time']:.2f}\"\n","            })\n","\n","        metrics_df = pd.DataFrame(metrics_data)\n","        print(\"MÉTRICAS DE DESEMPENHO:\")\n","        print(metrics_df.to_string(index=False))\n","\n","        # Melhor modelo\n","        best_model = max(self.results.items(), key=lambda x: x[1]['metrics']['auc_roc'])\n","        print(f\"\\n✨ MELHOR MODELO: {best_model[0]} (AUC-ROC: {best_model[1]['metrics']['auc_roc']:.4f})\")\n","\n","        # Hiperparâmetros\n","        print(\"\\n\" + \"-\"*80)\n","        print(\"HIPERPARÂMETROS OTIMIZADOS:\")\n","        print(\"-\"*80)\n","        for model_name, results in self.results.items():\n","            print(f\"\\n{model_name}:\")\n","            for param, value in results['best_params'].items():\n","                print(f\"  - {param}: {value}\")\n","\n","        # Insights\n","        print(\"\\n\" + \"-\"*80)\n","        print(\"INSIGHTS E RECOMENDAÇÕES:\")\n","        print(\"-\"*80)\n","\n","        # Análise por modelo\n","        for model_name, results in self.results.items():\n","            metrics = results['metrics']\n","            print(f\"\\n{model_name}:\")\n","\n","            if metrics['precision'] > metrics['recall']:\n","                print(\"  ✓ Alta precisão: Poucos falsos positivos\")\n","                print(\"  ⚠ Recall menor: Pode perder alguns casos de attrition\")\n","            else:\n","                print(\"  ✓ Alto recall: Captura bem os casos de attrition\")\n","                print(\"  ⚠ Precisão menor: Mais falsos alarmes\")\n","\n","            if metrics['auc_roc'] > 0.85:\n","                print(\"  ✓ Excelente capacidade de discriminação (AUC > 0.85)\")\n","            elif metrics['auc_roc'] > 0.75:\n","                print(\"  ✓ Boa capacidade de discriminação (AUC > 0.75)\")\n","            else:\n","                print(\"  ⚠ Capacidade de discriminação pode ser melhorada\")\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    def save_models(self, path='models/'):\n","        \"\"\"Salva os modelos treinados\"\"\"\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","\n","        # Salvar preprocessador\n","        joblib.dump(self.preprocessor, f'{path}preprocessor.pkl')\n","\n","        # Salvar modelos\n","        for model_name, model in self.models.items():\n","            filename = f'{path}{model_name.lower()}_model.pkl'\n","            joblib.dump(model, filename)\n","            print(f\"Modelo {model_name} salvo em: {filename}\")\n","\n","        # Salvar metadados\n","        metadata = {\n","            'feature_names': self.feature_names,\n","            'numeric_features': self.numeric_features,\n","            'categorical_features': self.categorical_features,\n","            'results': {k: v['metrics'] for k, v in self.results.items()},\n","            'training_date': datetime.now().isoformat()\n","        }\n","\n","        import json\n","        with open(f'{path}metadata.json', 'w') as f:\n","            json.dump(metadata, f, indent=2)\n","\n","        print(f\"\\nTodos os modelos salvos em: {path}\")"],"metadata":{"id":"PeH077nRK1PN","executionInfo":{"status":"ok","timestamp":1749766272724,"user_tz":180,"elapsed":60,"user":{"displayName":"Matheus Pavani","userId":"01895380900599612817"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 2. Criar pipeline\n","pipeline = HRAttritionPipeline(use_gpu=True)\n","\n","# 3. Preparar e treinar\n","X, y = pipeline.load_and_prepare_data(df)\n","X_eng = pipeline.create_features(X)\n","X_train, X_test, y_train, y_test = train_test_split(X_eng, y, test_size=0.2)\n","pipeline.train_models(X_train, y_train, X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485,"referenced_widgets":["6fd3631b0dd842e6b2e38e46bcc9a98b","d215c4a6bc8c450ea27372b2f965c49d","3fb1f0cea01249f6b26472ce09e897a1","a3c40744288c454fa40960bc8074b36e","b1afa80b5eb1473192976f4a2cc778df","9493285b49354e2c8255329d293a65f1","2f3fa9d2517c436db018f210bf2bd985","2f7b9b5a9d134390958c4b1b187b8fee","b969e081bf9943339f58a9872b737678","bb254f489758497ba9efdc0b379cd0b1","f2eb9930c22740b19f1e6a694861d508"]},"id":"fMkNj7IQLGlP","outputId":"17f169bc-569d-45d3-fe83-47436ba2a1ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Preparação dos Dados ===\n","Shape inicial: (1000000, 35)\n","Colunas removidas (valor único): ['EmployeeCount', 'Over18', 'StandardHours']\n","\n","Distribuição do target:\n","Attrition\n","0    0.821159\n","1    0.178841\n","Name: proportion, dtype: float64\n","\n","=== Feature Engineering ===\n","Features criadas: 14\n","Total de features: 44\n","\n","=== Treinamento dos Modelos ===\n","\n","Features numéricas: 35\n","Features categóricas: 9\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-12 22:11:40,378] A new study created in memory with name: no-name-479d077f-5628-405f-977b-1b91641a28a9\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- RandomForest ---\n","\n","=== Otimizando RandomForest ===\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/30 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd3631b0dd842e6b2e38e46bcc9a98b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[I 2025-06-12 22:13:25,662] Trial 0 finished with value: 0.6722532680858228 and parameters: {'n_estimators': 144, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.6722532680858228.\n"]}]}]}